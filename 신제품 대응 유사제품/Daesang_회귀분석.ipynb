{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "tutorial-mozambique",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-89d2b9903515>:6: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  from pandas import datetime\n",
      "C:\\Users\\chdus\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "# step0. import library\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "from matplotlib import pyplot\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, confusion_matrix, accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Auto model\n",
    "from auto_modelling.classification import GoClassify\n",
    "from auto_modelling.regression import GoRegress\n",
    "from auto_modelling.preprocess import DataManager\n",
    "from auto_modelling.stack import Stack\n",
    "import logging\n",
    "\n",
    "# Neural Network\n",
    "import keras\n",
    "import ast\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, LSTM, Bidirectional, Activation, Dropout\n",
    "from keras import backend as K\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "# save file\n",
    "import pickle\n",
    "import joblib\n",
    "from keras.models import model_from_json\n",
    "import re\n",
    "\n",
    "pd.set_option('display.max_rows', 500) # 많은 데이터를 한눈에 볼 수 있도록"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traditional-operator",
   "metadata": {},
   "source": [
    "# 회귀모델 (Auto_ML, 선형(OLS), 이항선형회귀(Logistic))\n",
    "\n",
    "아이템 별 월평균 데이터 셋 생성\n",
    "\n",
    "**_모델 종류_**\n",
    "- OLS: 선형회귀\n",
    "- MNLogit: 이항회귀\n",
    "- RandomForestRegressor\n",
    "- GoRegress: Auto_ML \n",
    "- GoClassify: Auto_ML\n",
    "- Neural_net: 신경망,Neural Network (분류)\n",
    "\n",
    "\n",
    "- 예측 모델의 입출력 값\n",
    "  - Input(x)은 신규 아이템에 대한 속성(이전에 _있던_ 속성 값)\n",
    "  - Output(y)는 기존 아이템에 대한 평균 QTY 예측값\n",
    "  \n",
    "\n",
    "### 모델을 파일로 저장\n",
    "\n",
    "- keras(케라스)를 쓰는 경우는 json / h5 파일로 모델 저장\n",
    "- sklearn를 쓰는 경우는 pickle 파일로 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fuzzy-north",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step1. Read data & data preparation\n",
    "read_data_file = 'new_DAESANG_DATA.csv'\n",
    "read_col_info_file = 'input_LSTM_데이터유형.csv' # 시계열로 변환이 필요한 예측주기는 일반회귀에선 사용 안함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "million-portal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions \n",
    "def read_data_info (read_data_file, read_col_info_file, read_model_info_file):\n",
    "    # input data\n",
    "    data = pd.read_csv(read_data_file) # 년도와 월을 split해서 new data 생성\n",
    "\n",
    "    # input role info\n",
    "    role_info = pd.read_csv(read_col_info_file, encoding ='cp949') # 모델링 할 때 사용할 x, y, month\n",
    "    x_val = role_info.loc[role_info['Role']=='x', 'col_name'].tolist() # x변수 다중리스트형태\n",
    "    y_val = role_info.loc[role_info['Role']=='y', 'col_name'].tolist()[0] # y변수는 단일\n",
    "    predic_period = role_info.loc[role_info['예측주기']=='P', 'col_name'].tolist()[0]\n",
    "    \n",
    "    # col info\n",
    "    dummy_list = role_info.loc[role_info['col_info']=='STR', 'col_name'].tolist() \n",
    "    \n",
    "    # input model info 모델과 예측달 정하기\n",
    "    model_info = pd.read_csv(read_model_info_file)\n",
    "    model_name = model_info['Model'][0] # 기본으로 auto_reg 지정\n",
    "    \n",
    "    # data split\n",
    "    Train = data[data['DATA_TYPE']=='TD'] # 학습할 데이터\n",
    "    Train[[y_val]] = Train[[y_val]].fillna(0).astype('int32') # 학습할 데이터\n",
    "    Predict = data[data['DATA_TYPE']=='RD'] # 예측해야할 데이터 \n",
    "    Predict[[y_val]] = Predict[[y_val]].fillna(0).astype('int32')\n",
    "    \n",
    "    return dummy_list, x_val, y_val, predic_period, Train, Predict, model_name\n",
    "\n",
    "\n",
    "def make_model_df (dummy_list, x_val, y_val, Train, Predict):\n",
    "    \n",
    "    # Train, Predict에서 role에 따른 값을 각각 X, y로 둠\n",
    "    X_Train = Train[x_val]\n",
    "    y_Train = Train[[y_val]].astype(int) # y_val은 값만 불러왔기에 이중리스트형태로 사용해야 dataframe형태로 출력\n",
    "    \n",
    "    X_Predict = Predict[x_val]\n",
    "    y_Predict = Predict[[y_val]]\n",
    "    #  y_Predict는 현재 없음, 있다면 실측값과 예측값을 비교해서 정확도 및 mse 확인 가능\n",
    "    \n",
    "    # 더미화 할 col정보\n",
    "    ele = [x for x in dummy_list if x in X_Train] # 더미화가 필요한 col중에 train에 들어가지 않는 것이 있을 수 있으므로 진행\n",
    "\n",
    "    # 더미화 형태의 X로 바꿈\n",
    "    X_Train = pd.get_dummies(data=X_Train, columns=ele)\n",
    "    X_Predict = pd.get_dummies(data=X_Predict, columns=ele)\n",
    "\n",
    "    return X_Train, y_Train, X_Predict, y_Predict\n",
    "\n",
    "# 빠른 결과값을 위해 일부로만 샘플진행\n",
    "train_num = 300\n",
    "test_num = 50\n",
    "\n",
    "\n",
    "def small_sample(train_num, test_num, Train, Predict):\n",
    "    # 빠른 결과값을 위해 일부로만 샘플진행\n",
    "    Train = Train.sample(n=train_num) # 300개\n",
    "    Predict = Predict.sample(n=test_num) # 50개\n",
    "\n",
    "    return Train, Predict\n",
    "\n",
    "def c_columns(df):\n",
    "    cols = []\n",
    "    cols.append('모델')\n",
    "    for i in range(len(df.columns)-1):\n",
    "        col = 'c'+str(i)\n",
    "        cols.append(col)\n",
    "    return cols\n",
    "\n",
    "def get_model_var(df,Model_ver):  #모델버전 생성 \n",
    "    df = df.reset_index(drop=False)\n",
    "    Model_ver_list =  [Model_ver] * len(df)\n",
    "    Model_ver_list = pd.DataFrame(Model_ver_list, columns =['모델 버전'])\n",
    "    \n",
    "    updated_df = pd.concat([Model_ver_list,df] ,axis=1)\n",
    "    return updated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "individual-pride",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "def outputfile(sheet1,output_file_name):     \n",
    "    sheet1.to_csv(output_file_name, encoding ='utf-8-sig')\n",
    "    print(\"\\n폴더에서\",output_file_name,\"파일을 확인하세요\")\n",
    "\n",
    "# 회귀 모델(선형, 로지스틱) 예측값 및 모델 파일로 출력\n",
    "def get_simple_results(model, X_Predict_df, y_val, Predict): # 식 결과 export into excel\n",
    "    model_results = model.summary()\n",
    "\n",
    "    model_info = model_results.tables[0].as_html()\n",
    "    model_info = pd.read_html(model_info, header=0, index_col=0)[0]\n",
    "    \n",
    "    model_result = model_results.tables[1].as_html()\n",
    "    model_result = pd.read_html(model_result, header=0, index_col=0)[0] # Excel 내보내기\n",
    "    \n",
    "#     return model_info, model_result\n",
    "    Model_ver = model_info[y_val][0] + \"_\" + model_info[y_val][1] + \"_\"+model_info[y_val][2]\n",
    "   \n",
    "    model_info_df = get_model_var(model_info,Model_ver)\n",
    "    model_info_df.columns = pd.MultiIndex.from_tuples(zip(c_columns(model_info_df), model_info_df.columns)) \n",
    "    \n",
    "    model_result_df = get_model_var(model_result,Model_ver)\n",
    "    model_result_df.columns = pd.MultiIndex.from_tuples(zip(c_columns(model_result_df), model_result_df.columns)) \n",
    "    \n",
    "    filename = re.sub('[<>.]','',str(model).split()[3])\n",
    "    model_file=\"{model}.pkl\".format(model=filename)\n",
    "    \n",
    "    joblib.dump(model, model_file)\n",
    "    clf_from_joblib = joblib.load(model_file)  \n",
    "    \n",
    "    prediction = clf_from_joblib.predict(X_Predict_df)\n",
    "    prediction_df = pd.DataFrame(data=prediction)\n",
    "    prediction_df.columns = ['Predicted']\n",
    "    prediction_df.index = X_Predict_df.index\n",
    "    \n",
    "    #prediction_df.index = range(len(prediction_df.index))\n",
    "    \n",
    "    results_pred_df= get_model_var(prediction_df, Model_ver)\n",
    "    results_pred_df_= results_pred_df.set_index('index').join(Predict)\n",
    "    results_pred_df_[y_val] = results_pred_df_['Predicted']\n",
    "    results_pred_df_ = results_pred_df_.drop(columns=['Predicted'])\n",
    "    \n",
    "    outputfile(results_pred_df_,output_file_name1) # 예측값 엑셀로 내보내기\n",
    "    outputfile(model_info_df,output_file_name2) # 모델 정보 엑셀로 내보내기\n",
    "    outputfile(model_result_df,output_file_name3) # 모델 식 엑셀로 내보내기\n",
    "    \n",
    "    return model_info_df,model_result_df,Model_ver,results_pred_df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "green-divide",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 회귀 모델(Auto_ML, 랜덤포레스트) 예측값 및 모델 파일로 출력\n",
    "def get_model_results (model, X_Predict_df):\n",
    "    \n",
    "    string_model = re.sub(\"\\n\",\"\",str(model)).replace(\" \",\"\")\n",
    "    suffix = pd.datetime.now().strftime(\"%y%m%d_%H%M%S\") # 파일이 돌아가기 시작한 시간을 기준으로 이름 생성\n",
    "    model_ = \"_\".join([suffix, string_model])\n",
    "    Model_ver = pd.DataFrame([model_], columns=['모델정보'])\n",
    "    \n",
    "    filename = model_.split('(',1)[0]\n",
    "    model_file=\"{model}.pkl\".format(model=filename)\n",
    "    \n",
    "    joblib.dump(model, model_file)\n",
    "    clf_from_joblib = joblib.load(model_file)  \n",
    "    \n",
    "    prediction = clf_from_joblib.predict(X_Predict_df) # 예측값구하는 식\n",
    "    prediction_df = pd.DataFrame(data=prediction)\n",
    "    prediction_df.columns = ['Predicted']\n",
    "    prediction_df.index = X_Predict_df.index\n",
    "    \n",
    "    results_pred_df= get_model_var(prediction_df, filename)    \n",
    "    \n",
    "    outputfile(Model_ver,output_file_name1)\n",
    "    outputfile(results_pred_df,output_file_name2)\n",
    "        \n",
    "    return Model_ver, results_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "amino-transfer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신경망 분석 모델 결과 출력\n",
    "def get_neural_results (model, X_Predict_df):\n",
    "    \n",
    "    Model_ver = pd.DataFrame([str(model)], columns=['모델정보'])\n",
    "    \n",
    "    filename = re.sub('[<>.]','',str(model).split()[3])\n",
    "    model_file1 = \"{model}.json\".format(model=filename)\n",
    "    model_file2 = \"{model}.h5\".format(model=filename)\n",
    "    \n",
    "    # serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(model_file1, \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(model_file2)\n",
    "\n",
    "    prediction = model.predict(X_Predict_df) # 예측값구하는 식\n",
    "    prediction_df = pd.DataFrame(data=prediction)\n",
    "    prediction_df.columns = ['Predicted']\n",
    "    prediction_df.index = X_Predict_df.index # scale하면 ndarray형태로 바껴서 index는 없음. (참고사항)\n",
    "    \n",
    "    results_pred_df= get_model_var(prediction_df, filename)    \n",
    "    \n",
    "    outputfile(Model_ver,output_file_name1)\n",
    "    outputfile(results_pred_df,output_file_name2)\n",
    "        \n",
    "    return Model_ver, results_pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pending-matthew",
   "metadata": {},
   "source": [
    "## 모델 선택 및 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "peripheral-contamination",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model select\n",
    "def main():\n",
    "    \n",
    "    # 데이터 분리\n",
    "    dummy_list, x_val, y_val, predic_period, Train, Predict, model_name = read_data_info (read_data_file, read_col_info_file, read_model_info_file)\n",
    "    \n",
    "    # 샘플뽑아 진행 (생략가능)\n",
    "    Train, Predict = small_sample(train_num, test_num, Train, Predict)\n",
    "    \n",
    "    # make dataset \n",
    "    X_Train_df, y_Train_df, X_Predict_df, y_Predict_df = make_model_df (dummy_list, x_val, y_val, Train, Predict)\n",
    "\n",
    "\n",
    "    # -------------- 모델 선택 -------------------\n",
    "    if model_name == 'logit': # 로지스틱 \n",
    "        \n",
    "        # 현재 dataset은 logit에 맞는 형태가 아니기에 임의로 변경해서 확인하는 작업입니다. \n",
    "        y_Train_df.loc[y_Train_df[y_val] > np.mean(y_Train_df[y_val]), y_val]=1\n",
    "        y_Train_df.loc[y_Train_df[y_val] > 1, y_val]=0\n",
    "    \n",
    "        model = sm.Logit(y_Train_df, X_Train_df).fit() \n",
    "        get_simple_results(model, X_Predict_df, y_val, Predict)\n",
    "        \n",
    "    elif model_name == 'MNlogit': # 다중 로지스틱\n",
    "        model = sm.MNLogit(y_Train_df, X_Train_df).fit() \n",
    "        get_simple_results(model, X_Predict_df, y_val, Predict)\n",
    "\n",
    "    elif model_name == 'OLS': # 선형회귀\n",
    "        model = sm.OLS(y_Train_df, X_Train_df).fit()\n",
    "        get_simple_results(model, X_Predict_df, y_val, Predict)\n",
    "        \n",
    "    elif model_name == 'Random_fore': # 랜덤포레스트\n",
    "        model = RandomForestRegressor(max_depth=2, random_state=0).fit(X_Train_df, y_Train_df) \n",
    "        get_model_results (model, X_Predict_df)\n",
    "        \n",
    "    # 현재 우리가 필요한 문제 auto_reg로 자동화 회귀모델링\n",
    "    # auto 모델의 경우 predict를 할 수 있는 reg와 분류작업을 위한 classifi를 직접 지정받아야하는 부분입니다. \n",
    "    elif model_name == 'Auto_classi':\n",
    "        model =  GoClassify(n_best=1).train(X_Train_df, y_Train_df)\n",
    "        get_model_results (model, X_Predict_df)\n",
    "        \n",
    "    elif model_name == 'Auto_reg':\n",
    "        model =  GoRegress(n_best=1).train(X_Train_df, y_Train_df)\n",
    "        get_model_results (model, X_Predict_df)\n",
    "        \n",
    "    # 신경망 (Deep learning)\n",
    "    elif model_name == 'Neural_net':\n",
    "\n",
    "        # scaling 하는 또다른 방법. 적용하였으면 추후 재 되돌리는 코드 필요. LSTM 코드 참조\n",
    "        #sc = StandardScaler()\n",
    "        #X_Train_df = sc.fit_transform(X_Train_df)\n",
    "        #y_Train_df = sc.fit_transform(y_Train_df)\n",
    "        #X_Predict_df = sc.fit_transform(X_Predict_df)\n",
    "        #X_Predict_df = sc.fit_transform(y_Predict_df)\n",
    "\n",
    "        # Initialising the ANN\n",
    "        model = Sequential()\n",
    "\n",
    "        # Adding the input layer and the first hidden layer\n",
    "        model.add(Dense(10, activation = 'relu', kernel_initializer='normal',  input_dim = X_Train_df.shape[1]))\n",
    "        \n",
    "        # Adding the second hidden layer\n",
    "        model.add(Dense(units = 8, activation = 'relu'))\n",
    "        # model.add(Dropout(0.5))\n",
    "        \n",
    "        # Adding the third hidden layer\n",
    "        # model.add(Dense(units = 4, activation = 'relu'))   #  레이어 추가\n",
    "        # model.add(Dropout(0.5))\n",
    "        \n",
    "        # Adding the output layer\n",
    "        model.add(Dense(units = 1, activation='relu'))\n",
    "        model.compile(optimizer = 'rmsprop',loss = 'mean_squared_error', metrics=['accuracy'])\n",
    "        model.fit(X_Train_df, y_Train_df, batch_size = 10, epochs = 150, verbose=0) # callback 안함. 필요시 LSTM 코드 참조 추가\n",
    "        \n",
    "        get_neural_results (model, X_Predict_df)\n",
    "        \n",
    "          \n",
    "    else: \n",
    "        print('Please select your data model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "linear-mailman",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "폴더에서 output_선형_예측값.csv 파일을 확인하세요\n",
      "\n",
      "폴더에서 output_선형_정보.csv 파일을 확인하세요\n",
      "\n",
      "폴더에서 output_선형_결과.csv 파일을 확인하세요\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chdus\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3427: DtypeWarning: Columns (2,5,6,9) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "C:\\Users\\chdus\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3191: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n"
     ]
    }
   ],
   "source": [
    "# 선형\n",
    "if __name__ == \"__main__\":  \n",
    "    read_model_info_file = 'input_AutoML_설정옵션2.csv'\n",
    "    output_file_name1 = 'output_선형_예측값.csv'\n",
    "    output_file_name2 = 'output_선형_정보.csv'\n",
    "    output_file_name3 = 'output_선형_결과.csv'\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "editorial-forward",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chdus\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3427: DtypeWarning: Columns (2,5,6,9) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "C:\\Users\\chdus\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3191: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.365800\n",
      "         Iterations 8\n",
      "\n",
      "폴더에서 output_로지스틱_예측값.csv 파일을 확인하세요\n",
      "\n",
      "폴더에서 output_로지스틱_정보.csv 파일을 확인하세요\n",
      "\n",
      "폴더에서 output_로지스틱_결과.csv 파일을 확인하세요\n"
     ]
    }
   ],
   "source": [
    "# 로지스틱\n",
    "if __name__ == \"__main__\":  \n",
    "    read_model_info_file = 'input_AutoML_설정옵션4.csv'\n",
    "    output_file_name1 = 'output_로지스틱_예측값.csv'\n",
    "    output_file_name2 = 'output_로지스틱_정보.csv'\n",
    "    output_file_name3 = 'output_로지스틱_결과.csv'\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "warming-treaty",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chdus\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3427: DtypeWarning: Columns (2,5,6,9) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "C:\\Users\\chdus\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3191: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n",
      "INFO:auto_modelling.regression:Starting to train models\n",
      "INFO:auto_modelling.regression:Starting to train with ExtraTreesRegressor\n",
      "C:\\Users\\chdus\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:740: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self.best_estimator_.fit(X, y, **fit_params)\n",
      "INFO:auto_modelling.regression:==============================================\n",
      "INFO:auto_modelling.regression:The current best result is -4845573.907710374\n",
      "INFO:auto_modelling.regression:with ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features=0.55, max_leaf_nodes=None,\n",
      "          min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "          min_samples_leaf=17, min_samples_split=16,\n",
      "          min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "          oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "INFO:auto_modelling.regression:==============================================\n",
      "INFO:auto_modelling.regression:Starting to train with GradientBoostingRegressor\n",
      "C:\\Users\\chdus\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "INFO:auto_modelling.regression:==============================================\n",
      "INFO:auto_modelling.regression:The current best result is -4845573.907710374\n",
      "INFO:auto_modelling.regression:with ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features=0.55, max_leaf_nodes=None,\n",
      "          min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "          min_samples_leaf=17, min_samples_split=16,\n",
      "          min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "          oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "INFO:auto_modelling.regression:==============================================\n",
      "INFO:auto_modelling.regression:Starting to train with AdaBoostRegressor\n",
      "C:\\Users\\chdus\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "INFO:auto_modelling.regression:==============================================\n",
      "INFO:auto_modelling.regression:The current best result is -4845573.907710374\n",
      "INFO:auto_modelling.regression:with ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features=0.55, max_leaf_nodes=None,\n",
      "          min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "          min_samples_leaf=17, min_samples_split=16,\n",
      "          min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "          oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "INFO:auto_modelling.regression:==============================================\n",
      "INFO:auto_modelling.regression:Starting to train with DecisionTreeRegressor\n",
      "INFO:auto_modelling.regression:==============================================\n",
      "INFO:auto_modelling.regression:The current best result is -4845573.907710374\n",
      "INFO:auto_modelling.regression:with ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features=0.55, max_leaf_nodes=None,\n",
      "          min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "          min_samples_leaf=17, min_samples_split=16,\n",
      "          min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "          oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "INFO:auto_modelling.regression:==============================================\n",
      "INFO:auto_modelling.regression:Starting to train with KNeighborsRegressor\n",
      "INFO:auto_modelling.regression:==============================================\n",
      "INFO:auto_modelling.regression:The current best result is -4845573.907710374\n",
      "INFO:auto_modelling.regression:with ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features=0.55, max_leaf_nodes=None,\n",
      "          min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "          min_samples_leaf=17, min_samples_split=16,\n",
      "          min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "          oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "INFO:auto_modelling.regression:==============================================\n",
      "INFO:auto_modelling.regression:Starting to train with RandomForestRegressor\n",
      "C:\\Users\\chdus\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:740: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self.best_estimator_.fit(X, y, **fit_params)\n",
      "INFO:auto_modelling.regression:==============================================\n",
      "INFO:auto_modelling.regression:The current best result is -4845573.907710374\n",
      "INFO:auto_modelling.regression:with ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features=0.55, max_leaf_nodes=None,\n",
      "          min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "          min_samples_leaf=17, min_samples_split=16,\n",
      "          min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "          oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "INFO:auto_modelling.regression:==============================================\n",
      "INFO:auto_modelling.regression:Starting to train with XGBRegressor\n",
      "INFO:auto_modelling.regression:==============================================\n",
      "INFO:auto_modelling.regression:The current best result is -4762406.581058623\n",
      "INFO:auto_modelling.regression:with XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "       importance_type='gain', interaction_constraints='',\n",
      "       learning_rate=0.1, max_delta_step=0, max_depth=6,\n",
      "       min_child_weight=2, missing=nan, monotone_constraints='()',\n",
      "       n_estimators=100, n_jobs=1, nthread=1, num_parallel_tree=1,\n",
      "       objective='reg:squarederror', random_state=0, reg_alpha=0,\n",
      "       reg_lambda=1, scale_pos_weight=1, subsample=0.1,\n",
      "       tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "INFO:auto_modelling.regression:==============================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "폴더에서 output_오토_모델.csv 파일을 확인하세요\n",
      "\n",
      "폴더에서 output_오토_예측값.csv 파일을 확인하세요\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-85fe54c59435>:5: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  suffix = pd.datetime.now().strftime(\"%y%m%d_%H%M%S\") # 파일이 돌아가기 시작한 시간을 기준으로 이름 생성\n"
     ]
    }
   ],
   "source": [
    "# Auto_ML\n",
    "if __name__ == \"__main__\":  \n",
    "    read_model_info_file = 'input_AutoML_설정옵션.csv'\n",
    "    output_file_name1 = 'output_오토_모델.csv'\n",
    "    output_file_name2 = 'output_오토_예측값.csv'\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "precious-maximum",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chdus\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3427: DtypeWarning: Columns (2,5,6,9) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "C:\\Users\\chdus\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3191: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "폴더에서 output_신경망_모델.csv 파일을 확인하세요\n",
      "\n",
      "폴더에서 output_신경망_예측값.csv 파일을 확인하세요\n"
     ]
    }
   ],
   "source": [
    "# 신경망 \n",
    "if __name__ == \"__main__\":  \n",
    "    read_model_info_file = 'input_AutoML_설정옵션3.csv'\n",
    "    output_file_name1 = 'output_신경망_모델.csv'\n",
    "    output_file_name2 = 'output_신경망_예측값.csv'\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defensive-accessory",
   "metadata": {},
   "source": [
    "## 파일에 저장한 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "comprehensive-literature",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_keras(jsonfile, h5file, new_RD): # 신경망\n",
    "    json_file = open(jsonfile, 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(h5file)\n",
    "    loaded_model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    \n",
    "    pred = loaded_model.predict(new_RD) # 예측값구하는 식\n",
    "    pred_df = pd.DataFrame(data=pred)\n",
    "    pred_df.columns = ['new_Predicted']\n",
    "    pred_df.index = new_RD.index\n",
    "    \n",
    "    outputfile(pred_df,output_file_name)\n",
    "    \n",
    "    return pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "younger-characterization",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_sklearn(filename, new_RD): # 모든 회귀모델 \n",
    "    clf_from_joblib = joblib.load(filename) \n",
    "    pred = clf_from_joblib.predict(new_RD)\n",
    "    pred_df = pd.DataFrame(data=pred)\n",
    "    pred_df.columns = ['new_Predicted']\n",
    "    pred_df.index = new_RD.index\n",
    "  \n",
    "    outputfile(pred_df,output_file_name)\n",
    "    \n",
    "    return pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "magnetic-vertical",
   "metadata": {},
   "source": [
    "## 저장된 파일로 모델 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "hourly-clone",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main2():\n",
    "    dummy_list, x_val, y_val, predic_period, Train, Predict, model_name = read_data_info (read_data_file, read_col_info_file, read_model_info_file)\n",
    "\n",
    "\n",
    "    # 샘플뽑아 진행 (생략가능)\n",
    "    Train, Predict = small_sample(300, 50, Train, Predict) # random하게 data를 뽑아와 모델 재사용해봄\n",
    "    # 샘플사이즈는 동일하지만, index는 임의적으로 지정되어 데이터 구성, 즉, 새로운 데이터를 만든 것\n",
    "\n",
    "    # make dataset \n",
    "    X_Train_df, y_Train_df, X_Predict_df, y_Predict_df = make_model_df (dummy_list, x_val, y_val, Train, Predict)\n",
    "    new_RD = X_Predict_df\n",
    "    \n",
    "    if model_name == 'logit' or model_name == 'OLS' or model_name == 'Auto_reg':\n",
    "        load_model_sklearn (file, new_RD)\n",
    "        \n",
    "    elif model_name == 'Neural_net' or model_name == 'Random_fore':\n",
    "        load_model_keras(jsonfile, h5file, new_RD)\n",
    "        \n",
    "    else: \n",
    "        print('Please select your data model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "discrete-triangle",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "폴더에서 output_선형_모델재사용_예측값.csv 파일을 확인하세요\n"
     ]
    }
   ],
   "source": [
    "# 선형\n",
    "if __name__ == \"__main__\":\n",
    "    read_model_info_file = 'input_AutoML_설정옵션2.csv'\n",
    "    file = '0x000001DAD4B9E8E0.pkl'\n",
    "    output_file_name = 'output_선형_모델재사용_예측값.csv'\n",
    "    main2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "scheduled-mediterranean",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "폴더에서 output_로지스틱_모델재사용_예측값.csv 파일을 확인하세요\n"
     ]
    }
   ],
   "source": [
    "# 로지스틱\n",
    "if __name__ == \"__main__\":\n",
    "    read_model_info_file = 'input_AutoML_설정옵션4.csv'\n",
    "    file = '0x000001DAC1BBF2E0.pkl'\n",
    "    output_file_name = 'output_로지스틱_모델재사용_예측값.csv'\n",
    "    main2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "approximate-plastic",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "폴더에서 output_AutoML_모델재사용_예측값.csv 파일을 확인하세요\n"
     ]
    }
   ],
   "source": [
    "# Auto_ML\n",
    "if __name__ == \"__main__\":\n",
    "    read_model_info_file = 'input_AutoML_설정옵션.csv'\n",
    "    file = '210324_145522_ExtraTreesRegressor.pkl'\n",
    "    output_file_name = 'output_AutoML_모델재사용_예측값.csv'\n",
    "    main2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "premium-third",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "폴더에서 output_신경망_모델재사용_예측값.csv 파일을 확인하세요\n"
     ]
    }
   ],
   "source": [
    "# 신경망\n",
    "if __name__ == \"__main__\":\n",
    "    read_model_info_file = 'input_AutoML_설정옵션3.csv'\n",
    "    jsonfile = '0x000001DAC49700A0.json' # 파일 이름 \n",
    "    h5file = '0x000001DAC49700A0.h5'\n",
    "    output_file_name = 'output_신경망_모델재사용_예측값.csv'\n",
    "    main2()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polish-corner",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "union-forge",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seeing-marina",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "veterinary-roulette",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "running-butter",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moving-vintage",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suburban-worry",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
