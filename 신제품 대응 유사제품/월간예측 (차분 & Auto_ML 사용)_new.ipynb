{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "after-defense",
   "metadata": {},
   "source": [
    "**Input 정보**\n",
    "- 데이터 셋 파일명: new_DAESANG_DATA.csv\n",
    "- 설정 옵션 파일명: input_AutoML_설정옵션.csv\n",
    "- 데이터 유형 파일명: input_AutoML_데이터유형.csv\n",
    "\n",
    "**모델정보**\n",
    "- auto_modelling\n",
    "- auto_modelling 처음 실행시 **!pip install auto_modelling** 코드 실행 필요\n",
    "\n",
    "\n",
    "**코드설명**\n",
    "- 시나리오 2-1. 월간예측 (차분 & Auto_ML 사용)\n",
    "    - 월별 RD 예측 (ID별예측, feature 조건별 예측)\n",
    "\n",
    "**Output 정보**\n",
    "- 예측 결과\n",
    "- 예측 모델 정보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "broadband-motivation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-503550ec574e>:5: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  from pandas import datetime\n",
      "C:\\Users\\chdus\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "from matplotlib import pyplot\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, confusion_matrix, accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from auto_modelling.classification import GoClassify\n",
    "from auto_modelling.regression import GoRegress\n",
    "from auto_modelling.preprocess import DataManager\n",
    "from auto_modelling.stack import Stack\n",
    "import logging\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose \n",
    "from pandas import DataFrame\n",
    "\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "import pmdarima\n",
    "from pmdarima import auto_arima\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Activation,Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import pickle\n",
    "import joblib\n",
    "from keras.models import model_from_json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agricultural-genesis",
   "metadata": {},
   "source": [
    "## step1. Read data & data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "moving-sound",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_data_file = 'new_DAESANG_DATA.csv'\n",
    "read_col_info_file = 'input_AutoML_데이터유형.csv' \n",
    "read_model_info_file = 'input_AutoML_설정옵션.csv' # 1: Auto , 2: OLS , 3: N.N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "french-purpose",
   "metadata": {},
   "source": [
    "### 1.1 기본 정보\n",
    "\n",
    "- 데이터 파일 읽기\n",
    "- 모델링에 필요한 데이터 유형, 역할, 예측 주기 파라미터 값 사용자 지정 파일에서 불러오기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "younger-embassy",
   "metadata": {},
   "source": [
    "1. 파일명 입력 -> 데이터 읽기/사용자 지정 조건들 읽기 (파라미터값들)\n",
    " \n",
    " \n",
    "2. 차분만 이용/ 회귀식 선택시 -> Split Train & Test data set\n",
    "\n",
    "\n",
    "    2.1 Run 회귀식/데이터모델\n",
    "\n",
    "    2.2 결과 출력\n",
    "\n",
    "\n",
    "3. 월간 예측 전처리 (1.2) 필요/ 차분 회귀식 선택  \n",
    "\n",
    "\n",
    "    3.1 피봇/더미화 (전처리)\n",
    " \n",
    "    3.2 분석모델 실행\n",
    " \n",
    "    3.3 결과 출력 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "technological-trading",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_info (read_data_file, read_col_info_file, read_model_info_file):\n",
    "    # input data\n",
    "    data = pd.read_csv(read_data_file) # 년도와 월을 split해서 new data 생성\n",
    "\n",
    "    # col info\n",
    "    col_info = data.iloc[0] # col 정보 ex. int / str / month_no / ...\n",
    "    dummy_list = col_info.loc[col_info=='STR'].index.tolist() # str부분 더미화처리해야함\n",
    "    id_val = col_info.loc[col_info=='STR_KEY'].index.tolist()[0] # key는 자동으로 id 인식\n",
    "\n",
    "    # input role info\n",
    "    role_info = pd.read_csv(read_col_info_file, encoding ='cp949') # 모델링 할 때 사용할 x, y, month\n",
    "    x_val = role_info.loc[role_info['Role']=='x', 'col_name'].tolist() # x변수 다중리스트형태\n",
    "    y_val = role_info.loc[role_info['Role']=='y', 'col_name'].tolist()[0] # y변수는 단일\n",
    "    month_val = role_info.loc[role_info['예측주기']=='p', 'col_name'].tolist()[0] # month값은 단일\n",
    "\n",
    "    # input model info 모델과 예측달 정하기\n",
    "    model_info = pd.read_csv(read_model_info_file)\n",
    "    model_name = model_info['Model'][0] # 기본으로 auto 지정\n",
    "    month_name = model_info['예측월'][0] # 예측하고 싶은 월 예:) 3월 QTY예측 / 10월 QTY 예측\n",
    "    # 맨처음 month로 불러온 것과 동일하게 이름작성 필수 ex. Mar (o) / March (X) / 3 (X)\n",
    "\n",
    "    # data split\n",
    "    Train = data[data['DATA_TYPE']=='TD'] # 학습할 데이터\n",
    "    Train[[y_val]] = Train[[y_val]].fillna(0).astype('int32') # 학습할 데이터\n",
    "    Predict = data[data['DATA_TYPE']=='RD'] # 예측해야할 데이터 \n",
    "    Predict[[y_val]] = Predict[[y_val]].fillna(0).astype('int32')\n",
    "    \n",
    "    return dummy_list, x_val, y_val, id_val, month_val, Train, Predict, model_name, month_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electronic-malta",
   "metadata": {},
   "source": [
    "### 1.2 전처리 과정 \n",
    "- 월간예측을 위한 전처리 과정\n",
    "- 예측하고 싶은 달에 대한 예측값 출력을 위한 전처리 과정\n",
    "- 완성된 Train, Predict data를 각각 role에 따라 분리한 후 col정보에 따른 더미화 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "celtic-underwear",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_df (dummy_list, x_val, y_val, id_val, month_val, Train, Predict, month_name):\n",
    "    \n",
    "    # part(id)에 따른 X, y train 할 값들 - 피봇이용해서 만들기\n",
    "    X_Train = pd.pivot_table(Train, index=id_val,values=x_val, aggfunc='first') # id에 따른 x 값 # 제품간 feature는 동일하므로 첫번째 값 출력\n",
    "    y_Train = pd.pivot_table(Train, index=id_val, columns=month_val, values=y_val, aggfunc = np.mean, fill_value = 0) # id에 따른 월별 예측을 위해 월별 평균/ 결측값은 0처리\n",
    "    X_Predict = pd.pivot_table(Predict, index=id_val,values=x_val, aggfunc='first') # Train과 col은 항상 일치해야함\n",
    "    #  y_Predict는 현재 없음, 있다면 실측값과 예측값을 비교해서 정확도 및 mse 확인 가능\n",
    "    y_Predict = pd.pivot_table(Predict, index=id_val, columns=month_val, values=y_val, aggfunc = np.mean, fill_value = 0)\n",
    "    \n",
    "    # 월별 qty를 위해 분리 # 모델불러올 때 예측하고 싶은 달 불러 올 것 - 아래참고\n",
    "    mons = []\n",
    "    for i in range(len(y_Train.columns)):\n",
    "        mon = pd.DataFrame(y_Train.iloc[:,i])\n",
    "        mons.append(mon)\n",
    "    for month in mons:\n",
    "        if month.columns == month_name:\n",
    "            month_df = pd.DataFrame(month)\n",
    "    # 즉, month가 예측하고 싶은 달이 됨\n",
    "\n",
    "    # 따라서 y_Train을 예측하고 싶은 것으로 재설정\n",
    "    y_Train = month_df # 월 QTY\n",
    "    \n",
    "    # y_Predict를 위해 같은 과정을 한번 더 해줌\n",
    "    mons2 = []\n",
    "    for i in range(len(y_Predict.columns)):\n",
    "        mon2 = pd.DataFrame(y_Predict.iloc[:,i])\n",
    "        mons2.append(mon2)\n",
    "    for month2 in mons2:\n",
    "        if month2.columns == month_name:\n",
    "            month2_df = pd.DataFrame(month2)\n",
    "\n",
    "    y_Predict = month2_df # 월 QTY\n",
    "    \n",
    "    # 차분을 더미화가 아닌 같다 아니다로 판별할 것이기에 필요 없음. 만약 TD의 column과 RD의 column이 일치하면 두가지방법(더미화 / 같음다름구별) 중 선택\n",
    "#     # 더미화 할 col정보\n",
    "#     ele = [x for x in dummy_list if x in X_Train] # 더미화가 필요한 col중에 train에 들어가지 않는 것이 있을 수 있으므로 진행\n",
    "\n",
    "#     # 더미화 형태의 X로 바꿈\n",
    "#     X_Train = pd.get_dummies(data=X_Train, columns=ele)\n",
    "#     X_Predict = pd.get_dummies(data=X_Predict, columns=ele)\n",
    "\n",
    "    return X_Train, y_Train, X_Predict, y_Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gorgeous-coordinate",
   "metadata": {},
   "source": [
    "## step2. 빠른 연산을 위한 단계 (선택사항) -> 전체 데이터로 모델링할시 step3로 이동\n",
    "\n",
    "- 파일 정보를 불러와 일부샘플을 뽑은 뒤 차분계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "exposed-mention",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 빠른 결과값을 위해 일부로만 샘플진행\n",
    "train_num = 300\n",
    "test_num = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "analyzed-response",
   "metadata": {},
   "outputs": [],
   "source": [
    "def small_sample(train_num, test_num, Train, Predict):\n",
    "    # 빠른 결과값을 위해 일부로만 샘플진행\n",
    "    Train = Train.sample(n=train_num) # 300개\n",
    "    Predict = Predict.sample(n=test_num) # 50개\n",
    "\n",
    "    return Train, Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "listed-shape",
   "metadata": {},
   "source": [
    "## step3. 차분계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "complimentary-landing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subtract_string(df):\n",
    "    df = df.transpose()\n",
    "    new_df = pd.DataFrame()\n",
    "    for i in range(len(df.columns)):\n",
    "        for j in range(i+1,len(df.columns)):\n",
    "            var = str(df.columns[i]) + '-' + str(df.columns[j])\n",
    "            dff_df = df.iloc[:,i].isin(df.iloc[:,j]).astype(int) # difference check 0 if True, 1 False\n",
    "            new_df[var] = np.logical_xor(dff_df,1).astype(int)  # XOR 다르면 1 같으면 0 for distance differentiation \n",
    "    \n",
    "    new_df_ = pd.DataFrame(data=new_df)\n",
    "    return new_df_.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "enabling-pendant",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subtract_integer(x):\n",
    "    x = x.transpose()\n",
    "    dff = pd.DataFrame()\n",
    "    for i in range(len(x.columns)):\n",
    "        for j in range(i+1,len(x.columns)):\n",
    "            var = str(x.columns[i]) + '-' + str(x.columns[j])\n",
    "            dff[var] = abs(x.iloc[:,i].astype('int32')-x.iloc[:,j].astype('int32'))\n",
    "            \n",
    "    temp_train_x = pd.DataFrame(data=dff)    \n",
    "    return temp_train_x.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "roman-contest",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subtract_btw_df_STR (new_test_df,train_df): # string \n",
    "    dff = pd.DataFrame()\n",
    "    \n",
    "    new_test_df = new_test_df.T\n",
    "    train_df = train_df.T\n",
    "    for i in range(len(new_test_df.columns)):\n",
    "        for j in range(len(train_df.columns)):\n",
    "            var = str(new_test_df.columns[i]) + '-' + str(train_df.columns[j])\n",
    "            dff_df = new_test_df.iloc[:,i].isin(train_df.iloc[:,j]).astype(int) # difference check 0 if True, 1 False\n",
    "            dff[var] = np.logical_xor(dff_df,1).astype(int)  # XOR 다르면 1 같으면 0 for distance differentiation \n",
    "    \n",
    "    temp_train_x = pd.DataFrame(data=dff) \n",
    "    return temp_train_x.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "confidential-angel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new data (X_Predict) 와 X_Train의 차분 계산\n",
    "def subtract_btw_df_INT (new_test_df,train_df):\n",
    "    dff = pd.DataFrame()\n",
    "    \n",
    "    new_test_df = new_test_df.T\n",
    "    train_df = train_df.T\n",
    "    for i in range(len(new_test_df.columns)):\n",
    "        for j in range(len(train_df.columns)):\n",
    "            var = str(new_test_df.columns[i]) + '-' + str(train_df.columns[j])\n",
    "            dff[var] = abs(new_test_df.iloc[:,i].astype('int32')-train_df.iloc[:,j].astype('int32'))\n",
    "    \n",
    "    temp_train_x = pd.DataFrame(data=dff)\n",
    "    \n",
    "    return temp_train_x.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "czech-snapshot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 차분 계산\n",
    "def get_diff_df(X_Train, X_Predict, y_Train, y_Predict):\n",
    "    \n",
    "    # [차분 과정1.] 회귀식을 위한 데이터 준비 \n",
    "    X_Train_df = subtract_string(X_Train) # 학습 데이터 차분 (subtracting inside) (row1 - row0)\n",
    "    X_Predict_df = subtract_btw_df_STR(X_Predict, X_Train) # 예측 데이터셋을 위한 차분  (df1[row1] - df2[row1])\n",
    "    y_Train_df = subtract_integer(y_Train) # 학습 데이터 y 에 대한 차분 (y[row1] - y[row0])\n",
    "    y_Predict_df = subtract_btw_df_INT(y_Predict, y_Train)\n",
    "    # 알아보기 쉽게 이름 재 구성 _df를 제외하고 사용해야 일반회귀모델링과 같은 main함수 사용가능\n",
    "    \n",
    "    return X_Train_df, y_Train_df, X_Predict_df, y_Predict_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "backed-mitchell",
   "metadata": {},
   "source": [
    "## step4. 파일 저장경로 및 파일 저장정보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "therapeutic-franklin",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_var(df,Model_ver):  #모델버전 생성 \n",
    "    df = df.reset_index(drop=False)\n",
    "    Model_ver_list =  [Model_ver] * len(df)\n",
    "    Model_ver_list = pd.DataFrame(Model_ver_list, columns =['모델 버전'])\n",
    "    \n",
    "    updated_df = pd.concat([Model_ver_list,df] ,axis=1)\n",
    "    return updated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "minute-pollution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "def outputfile(sheet1,output_file_name):     \n",
    "    sheet1.to_csv(output_file_name, encoding ='utf-8-sig')\n",
    "    print(\"\\n폴더에서\",output_file_name,\"파일을 확인하세요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "respiratory-fiber",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_match_prediction(y_pred_df):\n",
    "    indexs = pd.DataFrame(y_pred_df.index.str.split('-',1).tolist(),\n",
    "                                 columns = ['RD_index','TD_index'])\n",
    "    y_pred_df_ = y_pred_df.reset_index(drop=True)\n",
    "    result = pd.concat([indexs,y_pred_df_], axis=1)\n",
    "    return result   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "third-grammar",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_results(model, X_Predict_df, y_val, Predict): # 식 결과 export into excel\n",
    "    model_results = model.summary()\n",
    "\n",
    "    model_info = model_results.tables[0].as_html()\n",
    "    model_info = pd.read_html(model_info, header=0, index_col=0)[0]\n",
    "    \n",
    "    model_result = model_results.tables[1].as_html()\n",
    "    model_result = pd.read_html(model_result, header=0, index_col=0)[0] # Excel 내보내기\n",
    "    \n",
    "#     return model_info, model_result\n",
    "    Model_ver = model_info[y_val][0] + \"_\" + model_info[y_val][1] + \"_\"+model_info[y_val][2]\n",
    "   \n",
    "    model_info_df = get_model_var(model_info,Model_ver)\n",
    "    model_info_df.columns = pd.MultiIndex.from_tuples(zip(c_columns(model_info_df), model_info_df.columns)) \n",
    "    \n",
    "    model_result_df = get_model_var(model_result,Model_ver)\n",
    "    model_result_df.columns = pd.MultiIndex.from_tuples(zip(c_columns(model_result_df), model_result_df.columns)) \n",
    "    \n",
    "    filename = re.sub('[<>.]','',str(model).split()[3])\n",
    "    model_file=\"{model}.pkl\".format(model=filename)\n",
    "    \n",
    "    joblib.dump(model, model_file)\n",
    "    clf_from_joblib = joblib.load(model_file)  \n",
    "    \n",
    "    prediction = clf_from_joblib.predict(X_Predict_df) # 예측값구하는 식\n",
    "    prediction_df = pd.DataFrame(data=prediction)\n",
    "    prediction_df.columns = ['Predicted']\n",
    "    prediction_df.index = X_Predict_df.index\n",
    "    \n",
    "    #prediction_df.index = range(len(prediction_df.index))\n",
    "    \n",
    "    results_pred_df= get_model_var(prediction_df, Model_ver)\n",
    "    results_pred_df_= results_pred_df.set_index('index').join(Predict)\n",
    "    results_pred_df_[y_val] = results_pred_df_['Predicted']\n",
    "    results_pred_df_ = results_pred_df_.drop(columns=['Predicted'])\n",
    "    \n",
    "    outputfile(results_pred_df_,output_file_name1) # 예측값 엑셀로 내보내기\n",
    "    outputfile(model_info_df,output_file_name2) # 모델 정보 엑셀로 내보내기\n",
    "    outputfile(model_result_df,output_file_name3) # 모델 식 엑셀로 내보내기\n",
    "    \n",
    "    return model_info_df,model_result_df,Model_ver,results_pred_df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "committed-supplier",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측값 출력\n",
    "def get_model_results (model, X_Predict_df):\n",
    "    \n",
    "    string_model = re.sub(\"\\n\",\"\",str(model)).replace(\" \",\"\")\n",
    "    suffix = pd.datetime.now().strftime(\"%y%m%d_%H%M%S\") # 파일이 돌아가기 시작한 시간을 기준으로 이름 생성\n",
    "    model_ = \"_\".join([suffix, string_model])\n",
    "    Model_ver = pd.DataFrame([model_], columns=['모델정보'])\n",
    "    \n",
    "    filename = model_.split('(',1)[0]\n",
    "    model_file=\"{model}.pkl\".format(model=filename)\n",
    "    \n",
    "    joblib.dump(model, model_file)\n",
    "    clf_from_joblib = joblib.load(model_file)  \n",
    "    \n",
    "    prediction = clf_from_joblib.predict(X_Predict_df) # 예측값구하는 식\n",
    "    prediction_df = pd.DataFrame(data=prediction)\n",
    "    prediction_df.columns = ['Predicted']\n",
    "    prediction_df.index = X_Predict_df.index\n",
    "    \n",
    "    results_pred_df = index_match_prediction(prediction_df)\n",
    "    # results_pred_df_= get_model_var(prediction_df, Model_ver)    \n",
    "    \n",
    "    outputfile(Model_ver,output_file_name1)\n",
    "    outputfile(results_pred_df,output_file_name2)\n",
    "        \n",
    "    return Model_ver, results_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "extreme-occasions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neural_results (model, X_Predict_df):\n",
    "    \n",
    "    Model_ver = pd.DataFrame([str(model)], columns=['모델정보'])\n",
    "    \n",
    "    filename = re.sub('[<>.]','',str(model).split()[3])\n",
    "    model_file1 = \"{model}.json\".format(model=filename)\n",
    "    model_file2 = \"{model}.h5\".format(model=filename)\n",
    "    \n",
    "    # serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(model_file1, \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(model_file2)\n",
    "\n",
    "    prediction = model.predict(X_Predict_df) # 예측값구하는 식\n",
    "    prediction_df = pd.DataFrame(data=prediction)\n",
    "    prediction_df.columns = ['Predicted']\n",
    "    prediction_df.index = X_Predict_df.index # scale하면 ndarray형태로 바껴서 index는 없음. (참고사항)\n",
    "    \n",
    "    results_pred_df = index_match_prediction(prediction_df)\n",
    "    # results_pred_df_= get_model_var(prediction_df, Model_ver)    \n",
    "    \n",
    "    outputfile(Model_ver,output_file_name1)\n",
    "    outputfile(results_pred_df,output_file_name2)\n",
    "        \n",
    "    return Model_ver, results_pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eastern-variation",
   "metadata": {},
   "source": [
    "# RUN Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "proof-geometry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model select\n",
    "def main():\n",
    "    \n",
    "    # 데이터 분리\n",
    "    dummy_list, x_val, y_val, id_val, month_val, Train, Predict, model_name, month_name = read_data_info (read_data_file, read_col_info_file, read_model_info_file)\n",
    "    \n",
    "    # 샘플뽑아 진행 (생략가능)\n",
    "    Train, Predict = small_sample(train_num, test_num, Train, Predict)\n",
    "    \n",
    "    # make dataset \n",
    "    X_Train_df, y_Train_df, X_Predict_df, y_Predict_df = make_model_df (dummy_list, x_val, y_val, id_val, month_val, Train, Predict, month_name)\n",
    "    \n",
    "    # 차분공식\n",
    "    X_Train_df, y_Train_df, X_Predict_df, y_Predict_df = get_diff_df(X_Train_df, X_Predict_df, y_Train_df, y_Predict_df)\n",
    "    \n",
    "    # -------------- 모델 선택 -------------------\n",
    "    if model_name == 'logit': # 로지스틱 \n",
    "        \n",
    "        # 현재 dataset은 logit에 맞는 형태가 아니기에 임의로 변경해서 확인하는 작업입니다. \n",
    "        y_Train_df.loc[y_Train_df[y_val] > np.mean(y_Train_df[y_val]), y_val]=1\n",
    "        y_Train_df.loc[y_Train_df[y_val] > 1, y_val]=0\n",
    "    \n",
    "        model = sm.Logit(y_Train_df, X_Train_df).fit() \n",
    "        get_simple_results(model, X_Predict_df, y_val, Predict)\n",
    "        \n",
    "    elif model_name == 'MNlogit': # 다중 로지스틱\n",
    "        model = sm.MNLogit(y_Train_df, X_Train_df).fit() \n",
    "        get_simple_results(model, X_Predict_df, y_val, Predict)\n",
    "\n",
    "    elif model_name == 'OLS': # 선형회귀\n",
    "        model = sm.OLS(y_Train_df, X_Train_df).fit()\n",
    "        get_simple_results(model, X_Predict_df, y_val, Predict)\n",
    "        \n",
    "    elif model_name == 'Random_fore': # 랜덤포레스트\n",
    "        model = RandomForestRegressor(max_depth=2, random_state=0).fit(X_Train_df, y_Train_df) \n",
    "        get_model_results (model, X_Predict_df)\n",
    "        \n",
    "    # 현재 우리가 필요한 문제 auto_reg로 자동화 회귀모델링\n",
    "    # auto 모델의 경우 predict를 할 수 있는 reg와 분류작업을 위한 classifi를 직접 지정받아야하는 부분입니다. \n",
    "    elif model_name == 'Auto_classi':\n",
    "        model =  GoClassify(n_best=1).train(X_Train_df, y_Train_df)\n",
    "        get_model_results (model, X_Predict_df)\n",
    "        \n",
    "    elif model_name == 'Auto_reg':\n",
    "        model =  GoRegress(n_best=1).train(X_Train_df, y_Train_df)\n",
    "        get_model_results (model, X_Predict_df)\n",
    "        \n",
    "    # 신경망 (Deep learning)\n",
    "    elif model_name == 'Neural_net':\n",
    "\n",
    "        # scaling 하는 또다른 방법. 적용하였으면 추후 재 되돌리는 코드 필요. LSTM 코드 참조\n",
    "        #sc = StandardScaler()\n",
    "        #X_Train_df = sc.fit_transform(X_Train_df)\n",
    "        #y_Train_df = sc.fit_transform(y_Train_df)\n",
    "        #X_Predict_df = sc.fit_transform(X_Predict_df)\n",
    "        #X_Predict_df = sc.fit_transform(y_Predict_df)\n",
    "\n",
    "        # Initialising the ANN\n",
    "        model = Sequential()\n",
    "\n",
    "        # Adding the input layer and the first hidden layer\n",
    "        model.add(Dense(10, activation = 'relu', kernel_initializer='normal',  input_dim = X_Train_df.shape[1]))\n",
    "        \n",
    "        # Adding the second hidden layer\n",
    "        model.add(Dense(units = 8, activation = 'relu'))\n",
    "        # model.add(Dropout(0.5))\n",
    "        \n",
    "        # Adding the third hidden layer\n",
    "        # model.add(Dense(units = 4, activation = 'relu'))   #  레이어 추가\n",
    "        # model.add(Dropout(0.5))\n",
    "        \n",
    "        # Adding the output layer\n",
    "        model.add(Dense(units = 1, activation='relu'))\n",
    "        model.compile(optimizer = 'rmsprop',loss = 'mean_squared_error', metrics=['accuracy'])\n",
    "        model.fit(X_Train_df, y_Train_df, batch_size = 10, epochs = 150, verbose=0) # callback 안함. 필요시 LSTM 코드 참조 추가\n",
    "        \n",
    "        get_neural_results (model, X_Predict_df)\n",
    "        \n",
    "          \n",
    "    else: \n",
    "        print('Please select your data model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "tamil-textbook",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chdus\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3427: DtypeWarning: Columns (2,5,6,9) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "C:\\Users\\chdus\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3191: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n",
      "INFO:auto_modelling.regression:Starting to train models\n",
      "INFO:auto_modelling.regression:Starting to train with ExtraTreesRegressor\n",
      "C:\\Users\\chdus\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:740: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self.best_estimator_.fit(X, y, **fit_params)\n",
      "INFO:auto_modelling.regression:==============================================\n",
      "INFO:auto_modelling.regression:The current best result is -239202.42800248906\n",
      "INFO:auto_modelling.regression:with ExtraTreesRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "          max_features=0.9000000000000001, max_leaf_nodes=None,\n",
      "          min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "          min_samples_leaf=18, min_samples_split=8,\n",
      "          min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "          oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "INFO:auto_modelling.regression:==============================================\n",
      "INFO:auto_modelling.regression:Starting to train with GradientBoostingRegressor\n",
      "C:\\Users\\chdus\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "INFO:auto_modelling.regression:==============================================\n",
      "INFO:auto_modelling.regression:The current best result is -239202.42800248906\n",
      "INFO:auto_modelling.regression:with ExtraTreesRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "          max_features=0.9000000000000001, max_leaf_nodes=None,\n",
      "          min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "          min_samples_leaf=18, min_samples_split=8,\n",
      "          min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "          oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "INFO:auto_modelling.regression:==============================================\n",
      "INFO:auto_modelling.regression:Starting to train with AdaBoostRegressor\n",
      "C:\\Users\\chdus\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "INFO:auto_modelling.regression:==============================================\n",
      "INFO:auto_modelling.regression:The current best result is -238947.98125333042\n",
      "INFO:auto_modelling.regression:with AdaBoostRegressor(base_estimator=None, learning_rate=1.0, loss='linear',\n",
      "         n_estimators=100, random_state=None)\n",
      "INFO:auto_modelling.regression:==============================================\n",
      "INFO:auto_modelling.regression:Starting to train with DecisionTreeRegressor\n",
      "INFO:auto_modelling.regression:==============================================\n",
      "INFO:auto_modelling.regression:The current best result is -238947.98125333042\n",
      "INFO:auto_modelling.regression:with AdaBoostRegressor(base_estimator=None, learning_rate=1.0, loss='linear',\n",
      "         n_estimators=100, random_state=None)\n",
      "INFO:auto_modelling.regression:==============================================\n",
      "INFO:auto_modelling.regression:Starting to train with KNeighborsRegressor\n",
      "INFO:auto_modelling.regression:==============================================\n",
      "INFO:auto_modelling.regression:The current best result is -238947.98125333042\n",
      "INFO:auto_modelling.regression:with AdaBoostRegressor(base_estimator=None, learning_rate=1.0, loss='linear',\n",
      "         n_estimators=100, random_state=None)\n",
      "INFO:auto_modelling.regression:==============================================\n",
      "INFO:auto_modelling.regression:Starting to train with RandomForestRegressor\n",
      "C:\\Users\\chdus\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:740: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self.best_estimator_.fit(X, y, **fit_params)\n",
      "INFO:auto_modelling.regression:==============================================\n",
      "INFO:auto_modelling.regression:The current best result is -238947.98125333042\n",
      "INFO:auto_modelling.regression:with AdaBoostRegressor(base_estimator=None, learning_rate=1.0, loss='linear',\n",
      "         n_estimators=100, random_state=None)\n",
      "INFO:auto_modelling.regression:==============================================\n",
      "INFO:auto_modelling.regression:Starting to train with XGBRegressor\n",
      "INFO:auto_modelling.regression:==============================================\n",
      "INFO:auto_modelling.regression:The current best result is -238947.98125333042\n",
      "INFO:auto_modelling.regression:with AdaBoostRegressor(base_estimator=None, learning_rate=1.0, loss='linear',\n",
      "         n_estimators=100, random_state=None)\n",
      "INFO:auto_modelling.regression:==============================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "폴더에서 output_auto_모델.csv 파일을 확인하세요\n",
      "\n",
      "폴더에서 output_auto_예측값.csv 파일을 확인하세요\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-ec91573d1248>:5: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  suffix = pd.datetime.now().strftime(\"%y%m%d_%H%M%S\") # 파일이 돌아가기 시작한 시간을 기준으로 이름 생성\n"
     ]
    }
   ],
   "source": [
    "# Auto_ML\n",
    "if __name__ == \"__main__\":  \n",
    "    read_model_info_file = 'input_AutoML_설정옵션.csv'\n",
    "    output_file_name1 = 'output_auto_모델.csv'\n",
    "    output_file_name2 = 'output_auto_예측값.csv'\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranging-aviation",
   "metadata": {},
   "source": [
    "## 저장된 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "powered-ocean",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_keras(jsonfile, h5file, new_RD):\n",
    "    json_file = open(jsonfile, 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(h5file)\n",
    "    loaded_model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    \n",
    "    pred = loaded_model.predict(new_RD) # 예측값구하는 식\n",
    "    pred_df = pd.DataFrame(data=pred)\n",
    "    pred_df.columns = ['Predicted']\n",
    "    pred_df.index = new_RD.index\n",
    "    \n",
    "    results_pred_df = index_match_prediction(pred_df)\n",
    "    \n",
    "    outputfile(results_pred_df, output_file_name)\n",
    "    \n",
    "    return results_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ultimate-southwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_sklearn(filename, new_RD):\n",
    "    clf_from_joblib = joblib.load(filename) \n",
    "    pred = clf_from_joblib.predict(new_RD)\n",
    "    pred_df = pd.DataFrame(data=pred)\n",
    "    pred_df.columns = ['new_Predicted']\n",
    "    pred_df.index = new_RD.index\n",
    "    \n",
    "    results_pred_df = index_match_prediction(pred_df)\n",
    "\n",
    "    outputfile(results_pred_df, output_file_name)\n",
    "    \n",
    "    return results_pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indonesian-accounting",
   "metadata": {},
   "source": [
    "## new_RD 임의 생성 및 모델 저장 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "grateful-shore",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main2():\n",
    "\n",
    "    dummy_list, x_val, y_val, id_val, month_val, Train, Predict, model_name, month_name = read_data_info (read_data_file, read_col_info_file, read_model_info_file)\n",
    "    \n",
    "    # 샘플뽑아 진행 (생략가능)\n",
    "    Train, Predict = small_sample(train_num, test_num, Train, Predict)\n",
    "    \n",
    "    # make dataset \n",
    "    X_Train_df, y_Train_df, X_Predict_df, y_Predict_df = make_model_df (dummy_list, x_val, y_val, id_val, month_val, Train, Predict, month_name)\n",
    "    \n",
    "    # 차분공식\n",
    "    X_Train_df, y_Train_df, X_Predict_df, y_Predict_df = get_diff_df(X_Train_df, X_Predict_df, y_Train_df, y_Predict_df)\n",
    "    new_RD = X_Predict_df\n",
    "    \n",
    "    if model_name == 'logit' or model_name == 'OLS' or model_name == 'Auto_reg':\n",
    "        load_model_sklearn (file, new_RD)\n",
    "        \n",
    "    elif model_name == 'Neural_net' or model_name == 'Random_fore':\n",
    "        load_model_keras(jsonfile, h5file, new_RD)\n",
    "        \n",
    "    else: \n",
    "        print('Please select your data model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "suitable-adult",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chdus\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3427: DtypeWarning: Columns (2,5,6,9) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "C:\\Users\\chdus\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3191: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "폴더에서 load_auto_예측값.csv 파일을 확인하세요\n"
     ]
    }
   ],
   "source": [
    "# auto\n",
    "if __name__ == \"__main__\":\n",
    "    read_model_info_file = 'input_AutoML_설정옵션.csv'\n",
    "    file = \"210331_111416_AdaBoostRegressor.pkl\"\n",
    "    output_file_name = 'load_auto_예측값.csv'\n",
    "    main2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executed-tiffany",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intimate-debut",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standard-chemical",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
