{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deasang Time serise \n",
    "\n",
    "대상 데이터를 활용해 아래와 같은 결과 값을 구현하였습니다.\n",
    "\n",
    "\n",
    "**목적**\n",
    "1. 각 아이템의 과거 데이터로 2020년도 QTY 예측\n",
    "2. 신규 아이템에 대한 증감 예측\n",
    "3. 월평균으로 2020년도의 QTY 예측\n",
    "\n",
    "**시계열 모델정보**\n",
    "- _Autom_Arima_\n",
    "- _Autom_Arima_ 처음 실행시   **!pip install pmdarima** 코드 실행 필요\n",
    "\n",
    "**Input 정보**\n",
    "- 데이터 셋 파일명: DAESANG_DATA_prepared.csv\n",
    "- 설정 옵션 파일명: input_시계열모델_설정옵션.csv\n",
    "- 분석 조건 값 파일명: input_시계열모델_조건설정값.csv\n",
    "- 데이터 유형 파일명: input_시계열모델_데이터유형.csv\n",
    "\n",
    "**Output 정보**\n",
    "- 예측 결과\n",
    "- 예측 모델 정보\n",
    "- 예측 모델 값\n",
    "- 변수 중요도\n",
    "\n",
    "\n",
    "\n",
    "※ 자세한 Input정보는 같은 폴더에 있는 Readme를 참고 해주세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 개요\n",
    "\n",
    "### 시나리오 1. 시계열 분석\n",
    "- 예측 모델의 입출력 값\n",
    "  - Input(x)는 QTY의 과거 값\n",
    "  - Output(y)는 QTY의 미래 예측 값 \n",
    "\n",
    "\n",
    "### 시나리오 2. 유사제품 예측 (차분 & 랜덤포레스트 분석)\n",
    "- 예측 모델의 입출력 값\n",
    "  - Input(x)은 신규 아이템에 대한 속성(이전에 **_없던_** 속성 값)\n",
    "  - Output(y)는 신규 아이템에 대한 QTY값(예측대상)의 증감\n",
    "  \n",
    "  시나리오 2.1 월간예측 (차분 & Auto_ML 사용)\n",
    "   - 계절성 확인 목적\n",
    " \n",
    "### 시나리오 3. 월평균 예측 (선형분석)\n",
    "- 예측 모델의 입출력 값\n",
    "  - Input(x)은 신규 아이템에 대한 속성(이전에 **_있던_** 속성 값)\n",
    "  - Output(y)는 기존 아이템에 대한 평균 QTY 예측값 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pmdarima # have to install for Library # 처음 실행시 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요 라이브러리\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose \n",
    "from pandas import DataFrame\n",
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "import pmdarima\n",
    "from pmdarima import auto_arima\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor# Instantiate model with 1000 decision trees\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import linear_model\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 구현 함수\n",
    "def get_data_type(data_type_file):\n",
    "    data_type_df = pd.read_csv(data_type_file,encoding='cp949')\n",
    "    \n",
    "    x_val = data_type_df.loc[data_type_df['Role']=='x', 'col_name'].tolist() # x변수 다중리스트형태\n",
    "    y_val = data_type_df.loc[data_type_df['Role']=='y', 'col_name'].tolist()[0] # y변수는 단일\n",
    "    predic_period = data_type_df.loc[data_type_df['예측주기']=='P', 'col_name'].tolist()[0] # y변수는 단일\n",
    "    \n",
    "    dummy_list = data_type_df.loc[data_type_df['col_info']=='STR', 'col_name'].tolist()# y변수는 단일\n",
    "    \n",
    "    item_col = data_type_df.loc[data_type_df['col_info']=='STR_KEY', 'col_name'].tolist()# y변수는 단일\n",
    "    \n",
    "    return x_val, y_val, dummy_list,item_col[0],predic_period\n",
    "\n",
    "def get_variables(setting_file_name, condition_file_name):\n",
    "    \"\"\"\n",
    "    Input: setting_file_name(모델설정값파일이름), condition_file_name(조건설정값파일이름)\n",
    "    \n",
    "    Input 설정 파일에서 지정된 컬럼 명으로 설정 값 및 조건 값들을 불러옴\n",
    "    \n",
    "    return: Date지정 컬럼명, 타겟 컬럼명, 주기 값, 조건 값(리스트 )\n",
    "    \"\"\"\n",
    "    setting_df = pd.read_csv(setting_file_name,encoding='cp949')\n",
    "    date_col_name = setting_df.at[0, 'Date_col'] \n",
    "    target_name = setting_df.at[0, 'Target'] \n",
    "    p = setting_df.at[0, 'Period'] \n",
    "    \n",
    "    condition_df = pd.read_csv(condition_file_name,encoding='cp949')\n",
    "    keys = condition_df['condition_col_name'].tolist()\n",
    "    values = condition_df['condition'].tolist()\n",
    "    \n",
    "    condition_list = get_conditions(keys,values)\n",
    "    \n",
    "    return date_col_name, target_name, p, condition_list\n",
    "\n",
    "def change_col_to_int (df, col_name_change_to_int): \n",
    "    # 데이터 유형 정보가 담긴 row1이 string이라 계산값 (타겟) int전환 필요\n",
    "    df[col_name_change_to_int] = df[col_name_change_to_int].astype('int32')\n",
    "\n",
    "def get_conditions(keys,values):\n",
    "    \"\"\"\n",
    "    Input: keys(조건 컬럼명), values(조건)\n",
    "    \n",
    "    return: Dictionary 형태로 조건 컬럼명과 조건 \n",
    "    \"\"\"\n",
    "    values = [tryeval(x) for x in values]\n",
    "    values2 = [[_] for _ in values]\n",
    "    conditions = dict(zip(keys, values2))\n",
    "    return conditions\n",
    "\n",
    "def tryeval(val): # change str into int in a list \n",
    "    try:\n",
    "        val = ast.literal_eval(val)\n",
    "    except ValueError:\n",
    "        pass\n",
    "    return val\n",
    "\n",
    "def get_dataset(data_file_name,target_name):\n",
    "    \"\"\"\n",
    "    Input: data_file_name(데이터셋 파일이름),target_name(설정값에서 불러온 타겟명)\n",
    "    \n",
    "    파일읽고 df에 저장, 타겟값은 int로 지정, Train dataset, TestDataset 분리\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(data_file_name, encoding='cp949') # encoding은 한국어 load \n",
    "    df = df.drop([0]) # 데이터 유형은 실제 데이터 학습에 필요하지 않음으로 드롭\n",
    "    df.fillna(0, inplace=True) # RD에 있는 NAN 데이터를 0으로 바꿈 => 데이터 유형을 정수로 표현하기 위해 (데이터 셋 분리과정)\n",
    "    change_col_to_int(df,target_name)\n",
    "    \n",
    "    # TD, RD 분리 (학습 데이터 및 실험 데이터 생성)\n",
    "    train = df[df['DATA_TYPE']== 'TD']\n",
    "    test = df[df['DATA_TYPE']== 'RD']\n",
    "    \n",
    "    return df, train, test\n",
    "\n",
    "def get_model_var(df,Model_ver):  #모델버전 생성 \n",
    "    df = df.reset_index(drop=False)\n",
    "    Model_ver_list =  [Model_ver] * len(df)\n",
    "    Model_ver_list = pd.DataFrame(Model_ver_list, columns =['모델 버전'])\n",
    "    \n",
    "    updated_df = pd.concat([Model_ver_list,df] ,axis=1)\n",
    "    return updated_df\n",
    "\n",
    "def c_columns(df):\n",
    "    cols = []\n",
    "    cols.append('모델')\n",
    "    for i in range(len(df.columns)-1):\n",
    "        col = 'c'+str(i)\n",
    "        cols.append(col)\n",
    "    return cols\n",
    "\n",
    "def export_results(df_model): #시계열 식 결과 export into excel\n",
    "    model_results = df_model.summary()\n",
    "\n",
    "    model_info = model_results.tables[0].as_html()\n",
    "    model_info = pd.read_html(model_info, header=0, index_col=0)[0]\n",
    "    \n",
    "    model_result = model_results.tables[1].as_html()\n",
    "    model_result = pd.read_html(model_result, header=0, index_col=0)[0] # Excel 내보내기\n",
    "    \n",
    "    Model_ver = model_info['y'][0] + \"_\" + model_info['y'][1] + \"_\"+model_info['y'][2]\n",
    "   \n",
    "    model_info_df = get_model_var(model_info,Model_ver)\n",
    "    model_info_df.columns = pd.MultiIndex.from_tuples(zip(c_columns(model_info_df), model_info_df.columns)) \n",
    "    \n",
    "    model_result_df = get_model_var(model_result,Model_ver)\n",
    "    model_result_df.columns = pd.MultiIndex.from_tuples(zip(c_columns(model_result_df), model_result_df.columns)) \n",
    "    \n",
    "    return model_info_df,model_result_df,Model_ver\n",
    "\n",
    "def export_results_reg(df_model,y_pred_year,y_test,f1,f2,f3): #시계열 식 결과 export into excel\n",
    "    model_results = df_model.summary()\n",
    "\n",
    "    model_info = model_results.tables[0].as_html()\n",
    "    model_info = pd.read_html(model_info, header=0, index_col=0)[0]\n",
    "    \n",
    "    model_result = model_results.tables[1].as_html()\n",
    "    model_result = pd.read_html(model_result, header=0, index_col=0)[0] # Excel 내보내기\n",
    "    \n",
    "    Model_ver = model_info[y_val][0] + \"_\" + model_info[y_val][1] + \"_\"+model_info[y_val][2]\n",
    "   \n",
    "    model_info_df = get_model_var(model_info,Model_ver)\n",
    "    model_info_df.columns = pd.MultiIndex.from_tuples(zip(c_columns(model_info_df), model_info_df.columns)) \n",
    "    \n",
    "    model_result_df = get_model_var(model_result,Model_ver)\n",
    "    model_result_df.columns = pd.MultiIndex.from_tuples(zip(c_columns(model_result_df), model_result_df.columns)) \n",
    "    \n",
    "    prediction_df = pd.DataFrame(data=y_pred_year)\n",
    "    prediction_df.columns = ['예측값']\n",
    "    \n",
    "    results_pred_df = get_model_var(prediction_df,Model_ver)\n",
    "    results_pred_df_= results_pred_df.set_index('PART').join(y_test)\n",
    "    \n",
    "    outputfile(results_pred_df_,f1) # 예측값 엑셀로 내보내기\n",
    "    outputfile(model_info_df,f2) # 모델 정보 엑셀로 내보내기\n",
    "    outputfile(model_result_df,f3) # 모델 식 엑셀로 내보내기\n",
    "    \n",
    "    return model_info_df,model_result_df,Model_ver,results_pred_df_\n",
    "\n",
    "def outputfile(result1,output_file_name): # Export result into excel \n",
    "    result1.to_csv(output_file_name,encoding='utf-8-sig')\n",
    "    print(\"\\n▼ 폴더에서\",output_file_name,\"파일을 확인하세요\")\n",
    "    \n",
    "def export_to_excel(df_model,pred_df,f1,f2,f3):\n",
    "    model_info , model_result, Model_ver= export_results(df_model)\n",
    "    pred_df_ = get_model_var(pred_df,Model_ver)\n",
    "    outputfile(pred_df_,f1) # 예측값 엑셀로 내보내기\n",
    "    outputfile(model_info,f2) # 모델 정보 엑셀로 내보내기\n",
    "    outputfile(model_result,f3) # 모델 식 엑셀로 내보내기\n",
    "    \n",
    "def index_match_prediction(y_pred_df):\n",
    "    indexs = pd.DataFrame(y_pred_df.index.str.split('-',1).tolist(),\n",
    "                                 columns = ['RD_index','TD_index'])\n",
    "    y_pred_df_ = y_pred_df.reset_index(drop=True)\n",
    "    result = pd.concat([indexs,y_pred_df_], axis=1)\n",
    "    return result    \n",
    "    \n",
    "def filter_df(df, filter_values):\n",
    "    \"\"\"\n",
    "    Input: df(데이터셋), filter_values(Dic 형태의 조건 리스트: condition_list)\n",
    "    \n",
    "    Filter df by matching targets for multiple columns.\n",
    "    지정된 조건에 df 필터\n",
    "    \n",
    "    return 조건에 맞는 데이터셋 \n",
    "    \"\"\"\n",
    "    if filter_values is None or not filter_values:\n",
    "        return df\n",
    "    return df[\n",
    "        np.logical_and.reduce([\n",
    "            df[column].isin(target_values) \n",
    "            for column, target_values in filter_values.items()\n",
    "        ])\n",
    "    ]\n",
    "\n",
    "def find_similar_Item_ID(new_grouped1,item_col):\n",
    "    index_list = []\n",
    "    for i in range(len(new_grouped1)):\n",
    "        index = new_grouped1.index[i].split('-')[1]\n",
    "        index_list.append(int(index))\n",
    "        \n",
    "    item_list_by_index = []\n",
    "    for i in index_list:\n",
    "        item = df[df.index == i][item_col].values[0]\n",
    "        item_list_by_index.append(item)\n",
    "        \n",
    "    index_list_for_Predict = []\n",
    "    for i in range(len(new_grouped1)):\n",
    "        index = new_grouped1.index[i].split('-')[0]\n",
    "        index_list_for_Predict.append(int(index))\n",
    "        \n",
    "    item_list_for_Predict = []\n",
    "    for i in index_list_for_Predict:\n",
    "        item = df[df.index == i][item_col].values[0]\n",
    "        item_list_for_Predict.append(item)\n",
    "    \n",
    "    return index_list,item_list_by_index,index_list_for_Predict,item_list_for_Predict\n",
    "\n",
    "\n",
    "def runAutoArima_Prediction(train,test, target_name, date_col_name, condition, p):\n",
    "    \"\"\"\n",
    "    Input: train(학습용 데이터 셋), test(예측용 데이터 셋),\n",
    "    target_name (예측변수 - 타겟), date_col_name (Time 컬럼\n",
    "    ) ,\n",
    "    condition: 조건 리스트, p(예측 주기)\n",
    "    \n",
    "    Output = 시계열 모델식, 예측 데이터 셋\n",
    "    \"\"\"\n",
    "    df_train = filter_df(train, condition)\n",
    "    df_test = filter_df(test, condition)\n",
    "    \n",
    "    Q_train = df_train[[target_name,date_col_name]]\n",
    "    Q_test = df_test[[target_name,date_col_name]]\n",
    "    \n",
    "    Q_train[[target_name] ] = Q_train[[target_name]].astype('int32')\n",
    "    Q_test[[target_name] ] = Q_test[[target_name]].astype('int32')\n",
    "    \n",
    "    Q_train = Q_train.set_index(date_col_name)\n",
    "    Q_test = Q_test.set_index(date_col_name)\n",
    "    \n",
    "    n_p = len(df_test) # 데이터에 RD가 없는 경우 확인\n",
    "    \n",
    "    if n_p < 0: # If there are RD\n",
    "        print(\"예측기간이 정해지지 않았습니다.\")\n",
    "        \n",
    "    else:\n",
    "        stepwise_model_series = auto_arima(Q_train,m=p,seasonal = True)\n",
    "        stepwise_model_series.fit(Q_train)\n",
    "        future_forecast_ = stepwise_model_series.predict(n_periods= n_p)\n",
    "        future_forecast_ = pd.DataFrame(future_forecast_,index = Q_test.index,columns=[target_name])\n",
    "        if len(condition_list) <= 1:\n",
    "            pd.concat([Q_train,future_forecast_],axis=1).plot(figsize=(20,5))\n",
    "            prediected_df = pd.concat([Q_train,future_forecast_]) \n",
    "        \n",
    "        df_test[target_name] = future_forecast_[[target_name]].values\n",
    "        \n",
    "    return stepwise_model_series, df_test\n",
    "\n",
    "# 각 아이템 별 시계열 분석\n",
    "def Prediction_per_item(train,test,item_col,item, target_name, date_col_name, p):\n",
    "    \"\"\"\n",
    "    Input: train(학습용 데이터 셋), test(예측용 데이터 셋), itme_col (아이템 컬럼 리스트), item(아이템 명)\n",
    "    target_name(Y 데이터 컬럼 명), date_col_name(시계열 컬럼 명), p(예측 주기)\n",
    "    \"\"\"\n",
    "    df_train = train[train[item_col]== item]\n",
    "    df_test = test[test[item_col]== item]\n",
    "    \n",
    "    Q_train = df_train[[target_name,date_col_name]]\n",
    "    Q_test = df_test[[target_name,date_col_name]]\n",
    "    \n",
    "    Q_train[[target_name] ] = Q_train[[target_name]].astype('int32')\n",
    "    Q_test[[target_name] ] = Q_test[[target_name]].astype('int32')\n",
    "    \n",
    "    Q_train = Q_train.set_index(date_col_name)\n",
    "    Q_test = Q_test.set_index(date_col_name)\n",
    "    \n",
    "    n_p = len(df_test) # 데이터에 RD가 없는 경우 확인\n",
    "    \n",
    "    if n_p < 0: # If there are RD\n",
    "        print(\"예측기간이 정해지지 않았습니다.\")\n",
    "        \n",
    "    else:\n",
    "        stepwise_model_series = auto_arima(Q_train,m=p, max_D =1 ,seasonal = True)\n",
    "        stepwise_model_series.fit(Q_train)\n",
    "        future_forecast_ = stepwise_model_series.predict(n_periods= n_p)\n",
    "        future_forecast_ = pd.DataFrame(future_forecast_,index = Q_test.index,columns=[target_name])\n",
    "        pd.concat([Q_train,future_forecast_],axis=1).plot(figsize=(20,5))\n",
    "        prediected_df = pd.concat([Q_train,future_forecast_]) \n",
    "        print('아이템 컬럼: ',item_col,\", 아이템 명: \",item)\n",
    "        print(prediected_df)\n",
    "        \n",
    "    return prediected_df,stepwise_model_series\n",
    "\n",
    "# new data (X_Predict) 와 X_Train의 차분 계산\n",
    "def subtract (new_test_df,train_df):\n",
    "    dff = pd.DataFrame()\n",
    "    for i in range(len(new_test_df.columns)):\n",
    "        for j in range(len(train_df.columns)):\n",
    "            var = str(new_test_df.columns[i]) + '-' + str(train_df.columns[j])\n",
    "            dff[var] = abs(new_test_df.iloc[:,i].astype('int32')-train_df.iloc[:,j].astype('int32'))\n",
    "    \n",
    "    temp_train_x = pd.DataFrame(data=dff)\n",
    "    \n",
    "    return temp_train_x.T\n",
    "\n",
    "def get_subtract_sample(x):\n",
    "    x = x.transpose()\n",
    "    dff = pd.DataFrame()\n",
    "    for i in range(len(x.columns)):\n",
    "        for j in range(i+1,len(x.columns)):\n",
    "            var = str(x.columns[i]) + '-' + str(x.columns[j])\n",
    "            dff[var] = abs(x.iloc[:,i].astype('int32')-x.iloc[:,j].astype('int32'))\n",
    "            \n",
    "    temp_train_x = pd.DataFrame(data=dff)\n",
    "    #temp_train_x_0 = temp_train_x.fillna(0)\n",
    "    \n",
    "    return temp_train_x.T\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  데이터 불러오기 및 데이터 준비  (공통)\n",
    "#### 2.1 설정 값 파일 읽기 (사용 된 함수)\n",
    "   - get_variables() : 날자 컬럼, 타겟 컬럼, 주기, 조건 리스트\n",
    "   -  get_dataset()  : Train & Test 데이터 세트 준비\n",
    "   - get_data_type() : x변수, y변수, 명목변수, key이름, 주기 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input 파일 \n",
    "data_file_name = 'DAESANG_DATA_prepared.csv'\n",
    "setting_file_name = 'input_시계열모델_설정옵션.csv'\n",
    "data_type_file = 'input_시계열모델_데이터유형.csv'\n",
    "condition_file_name = 'input_시계열모델_조건설정값.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 시나리오 1. 시계열 분석\n",
    "runAutoArima_Prediction(): Auto Arima 실행 \n",
    "   - 조건별 RD 예측\n",
    "       - 시나리오 1.1 아이템별 y값 예측\n",
    "       - 시나리오 1.2 Feature 조건별 y값 예측\n",
    "       \n",
    "       \n",
    "Output▶ 예측값 및 모델 정보 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_col_name, target_name, p, condition_list = get_variables(setting_file_name, condition_file_name)\n",
    "df, train, test = get_dataset(data_file_name,target_name) #df: raw data, train:TD test:RD\n",
    "x_val, y_val, dummy_list,item_col,predic_period = get_data_type(data_type_file) # x변수, y변수, 명목변수, key이름, 주기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 시나리오 1.1 아이템 별 y 값 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART: 1019260 예측\n",
    "df_model, pred_df = runAutoArima_Prediction(train,test, target_name, date_col_name, condition_list, p)\n",
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_1 = \"output_시계열모델_예측결과.csv\" # 예측값 엑셀로 내보내기\n",
    "file_name_2 = \"output_시계열모델_정보.csv\"     # 모델 정보 엑셀로 내보내기\n",
    "file_name_3 = \"output_시계열모델_결과.csv\"     # 모델 식 엑셀로 내보내기\n",
    "# 결과 엑셀로 내보내기\n",
    "export_to_excel(df_model,pred_df, file_name_1,file_name_2,file_name_3) # 예측값,모델 정보, 모델 식 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 시나리오 1.2  조건별 y값 예측\n",
    "- 조건: 3월 냉동식품 B2C A급"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_file_name = 'input_시계열모델_조건설정값2.csv' \n",
    "date_col_name, target_name, p, condition_list = get_variables(setting_file_name, condition_file_name)\n",
    "df, train, test = get_dataset(data_file_name,target_name)\n",
    "train_month = filter_df(train, condition_list)\n",
    "test_month = filter_df(test, condition_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_model, pred_df = runAutoArima_Prediction(train_month,test_month,target_name, date_col_name, condition_list, p)\n",
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_name_1 = \"output_시계열모델_A급냉동3월_예측결과.csv\" # 예측값 엑셀로 내보내기\n",
    "file_name_2 = \"output_시계열모델_A급냉동3월_정보.csv\"     # 모델 정보 엑셀로 내보내기\n",
    "file_name_3 = \"output_시계열모델_A급냉동3월_결과.csv\"     # 모델 식 엑셀로 내보내기\n",
    "# 결과 엑셀로 내보내기\n",
    "export_to_excel(df_model,pred_df, file_name_1,file_name_2,file_name_3) # 예측값,모델 정보, 모델 식 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 시나리오 2. 유사제품 예측 (차분 & 랜덤포레스트 분석)\n",
    "\n",
    "\n",
    "- 2월 제품 중 상온이지만 판매 방식, 등급이 다른 4가지 제품의 QTY 예측\n",
    "- 비슷한 제품 찾기 (비슷제품 1)\n",
    "\n",
    "**가정:**   QTY를 예측하고 싶은 제품A는 기존 제품B와 특성이 비슷\n",
    "\n",
    "**∴ _제품A_ 는 _기존 제품B_ 의 예측값이랑 비슷할 것이다**\n",
    "- 시나리오 2.1 데이터 준비 및 더미화\n",
    "- 시나리오 2.2 모델 실행 및 유사성 예측값 출력\n",
    "- (참고1) 유사제품 순위\n",
    "- (참고2) 유사제품 시계열 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. 데이터 준비 및 더미화\n",
    "- 지정된 x와 y로 데이터셋 구성\n",
    "    - TD: x_train, y_train \n",
    "    - RD: x_predic, y_pred(예측값)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실험을 위한 파라미터 \n",
    "samplesize = 200\n",
    "num_of_prediction = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_file_name = 'input_시계열모델_조건설정값3.csv'\n",
    "date_col_name, target_name, p, condition_list = get_variables(setting_file_name, condition_file_name)\n",
    "df, train, test = get_dataset(data_file_name,target_name)\n",
    "dummies = [x for x in dummy_list if x in x_val] \n",
    "mon = condition_list.get(predic_period)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 더미화\n",
    "try_df = pd.get_dummies(data=df[dummies])\n",
    "try_df_ = pd.concat([try_df, df[[predic_period]],df[['DATA_TYPE']],df[[y_val]]],axis=1)\n",
    "\n",
    "x_train_dummified = try_df_.loc[(try_df_[predic_period]==mon) & (try_df_['DATA_TYPE']=='TD')]\n",
    "x_test_dummified = try_df_.loc[(try_df_[predic_period]==mon) & (try_df_['DATA_TYPE']=='RD')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 실험용\n",
    "x_train_dummified = x_train_dummified[x_train_dummified['FEATURE_1_상온'] == 1] # 빠른 체크를 위한 상온만 필터\n",
    "x_test_dummified = try_df_.loc[(try_df_[predic_period]==mon) & (try_df_['DATA_TYPE']=='RD')].head(num_of_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. 학습용 데이터셋 준비 (차분과정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train_dummified.drop(['MONTH','DATA_TYPE'], axis=1) # drop month and data_type (QTY는 유지)\n",
    "\n",
    "x_pred = x_test_dummified.drop(['MONTH','DATA_TYPE'], axis=1)\n",
    "y_pred = x_test_dummified[[y_val]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실험용\n",
    "x_train = x_train.sample(n=samplesize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1 학습용 차분\n",
    "\n",
    "**get_subtract_sample()** 로 차분된 데이터셋 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dummied_sub_self = get_subtract_sample(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dummied_sub_self.head() # 차분된 데이터 셋 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델링을 위한 데이터 구분\n",
    "x_train_dummied_sub_self = train_dummied_sub_self.drop([y_val], axis=1)\n",
    "y_train_dummied_sub_self = train_dummied_sub_self[[y_val]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2 예측용 데이터셋 차분\n",
    "\n",
    "**subtract()** 로 예측 데이터와 학습 데이터 차분\n",
    "- x_pred 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_predict_data_sub =subtract(x_pred.T, x_train.T)\n",
    "x_predict_data_sub_ = x_predict_data_sub.drop([y_val], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 시나리오 2.3 모델 실행 및 유사성 예측값 출력\n",
    "**RandomForestRegressor(X_Train_df,y_Train_df)** 랜덤포레스트 모델 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_model = RandomForestRegressor(max_depth=2, random_state=0).fit(x_train_dummied_sub_self, y_train_dummied_sub_self)\n",
    "y_pred= RF_model.predict(x_predict_data_sub_)\n",
    "y_pred_df = pd.DataFrame(data=y_pred)\n",
    "y_pred_df.columns = ['Predicted']\n",
    "y_pred_df.index = x_predict_data_sub_.index\n",
    "y_pred_df_ =  index_match_prediction(y_pred_df) # 결과값\n",
    "y_pred_df_ #RD_index would be smilar to TD_index, presucted by subtracted distance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_1 = \"output_랜덤포레스트_차분_유사성결과.csv\" # 유사값 엑셀로 내보내기\n",
    "\n",
    "# 결과 엑셀로 내보내기\n",
    "outputfile(y_pred_df_,file_name_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (참고) 유사성 랭크 순위 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_list = list(range(num_of_prediction)) # size of RD\n",
    "res =  [ele for ele in group_list for i in range(samplesize)]\n",
    "res1 =  pd.DataFrame(res, columns = ['Group'], index = y_pred_df.index) \n",
    "Grouped = pd.concat([res1,y_pred_df ],axis=1) \n",
    "# 제품간의 차분 거리가 가장 작은 top1 \n",
    "result_top1 = Grouped.drop_duplicates().groupby('Group', group_keys=False).apply(lambda x: x.sort_values('Predicted', ascending=True)).groupby('Group').head(1)\n",
    "result_top1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제품간의 차분 거리가 가장 작은 top5 \n",
    "result_top5 = Grouped.drop_duplicates().groupby('Group', group_keys=False).apply(lambda x: x.sort_values('Predicted', ascending=True)).groupby('Group').head(5)\n",
    "result_top5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (참고) 유사제품의 시계열 예측 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping PART with index\n",
    "index_list,item_list_by_index,index_list_for_Predict,item_list_for_Predict = find_similar_Item_ID(result_top1,item_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(item_list_by_index)):\n",
    "    item = item_list_by_index[i]\n",
    "    y_pred_by_Item, df_model_by_item = Prediction_per_item (train,test,item_col,item, y_val, date_col_name, p)\n",
    "    print(\"예측 item:\",item_list_for_Predict[i], \",   기존 item\", item_list_by_index[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 시나리오 3. 월평균 예측 (선형분석)\n",
    "\n",
    "    Y= c + a1.X1 + a2.X2 + a3.X3 + a4.X4 +a5X5 +a6X6 ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 아이템별 월평균 데이터 셋 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_col_to_int(train, 'YEAR')\n",
    "x_train = pd.pivot_table(train, index=item_col,values=x_val, aggfunc='first')\n",
    "x_train_dumm = pd.get_dummies(data=x_train)\n",
    "y_train = train[['PART','QTY']].groupby('PART').mean()\n",
    "\n",
    "y_test = pd.pivot_table(test, index=item_col,values=x_val, aggfunc='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  선형 모델실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lin_reg = sm.OLS(y_train, x_train_dumm).fit()\n",
    "\n",
    "x_predict = pd.pivot_table(test, index=item_col,values=x_val, aggfunc='first')\n",
    "x_predict_dumm = pd.get_dummies(data=x_predict)\n",
    "\n",
    "y_pred_year = lin_reg.predict(x_predict_dumm)\n",
    "lin_reg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_1 = \"output_월평균_선형_예측.csv\" # 예측값 엑셀로 내보내기\n",
    "file_name_2 = \"output_월평균_선형_예측모델_정보.csv\"     # 모델 정보 엑셀로 내보내기\n",
    "file_name_3 = \"output_월평균_선형_예델모델_결과.csv\"     # 모델 식 엑셀로 내보내기\n",
    "model_info_df,model_result_df,Model_ver,results_pred_df = export_results_reg(lin_reg,y_pred_year,y_test,file_name_1,file_name_2,file_name_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (참고: 중요도 및 상관분석)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_= x_train_dumm.join(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Univariate Selection : 통계 테스트를 통해 변수들 간의 관계 이해 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# apply SelectKBest class to extract top 10 best features\n",
    "bestfeatures = SelectKBest(score_func=chi2, k=10)\n",
    "fit = bestfeatures.fit(train_.iloc[:,:-1],train_.iloc[:,0])\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(train_.iloc[:,:-1].columns)\n",
    "#concat two dataframes for better visualization \n",
    "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "featureScores.columns = ['Features','Score']  #naming the dataframe columns\n",
    "featureScores_df = featureScores.sort_values(by=['Score'], ascending=False)\n",
    "featureScores_df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature importance : 변수 중요도 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ExtraTreesClassifier()\n",
    "fit = model.fit(train_.iloc[:,:-1],train_.iloc[:,0])\n",
    "#print(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n",
    "#plot graph of feature importances for better visualization\n",
    "feat_importances = pd.Series(model.feature_importances_, index=train_.iloc[:,:-1].columns)\n",
    "clrs = ['grey' if (x < max(model.feature_importances_)) else 'blue' for x in model.feature_importances_ ]\n",
    "feat_importances.nlargest(10).plot(kind='barh',color=clrs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances_df = pd.DataFrame(model.feature_importances_, index=train_.iloc[:,:-1].columns)\n",
    "feature_importances_df.columns = ['Score']\n",
    "feature_importances_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### x 변수들의 상관분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmat = train_.corr()\n",
    "top_corr_features = corrmat.index\n",
    "plt.figure(figsize=(13,13))\n",
    "#plot heat map\n",
    "g=sns.heatmap(train_[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
